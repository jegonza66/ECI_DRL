{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación del código\n",
    "\n",
    "Se debe contener \"dqn_agent.py\" y \"model.py\" en la carpeta donde se encuentra el archivo \"Tp.ipynb\". Los mismos contienen la configuración de la red neuronal utilizada, y del agente que se entrenará. En este caso la red tiene dos capas ocultas de 64 nodos con una función de activación Relu a la salida de cada una. Por otro lado el archivo correspondiente a la configuración del agente tiene en el principio la configuración de ciertos parametros como el \"gamma\" el cual toma un valor de 0.99, y el \"lr\" el cual vale 0.0005. \n",
    "\n",
    "## Creación del entorno\n",
    "\n",
    "Se importa de la biblioteca gfootball el los entornos correspondientes y se crea el entorno a utilizar (en este caso \" academy_empty_goal_close\") con las configuraciones iniciales (como recompensa \"checkpoint\" que recompensa antes de marcar para facilitar el progreso).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gfootball.env as football_env\n",
    "env = football_env.create_environment(\n",
    "    env_name='academy_empty_goal_close', \n",
    "    stacked=False,                           # solo estado, no pixeles \n",
    "    representation='simple115',              # solo estado, no pixeles \n",
    "    with_checkpoints=True,                   # recompensas intermedias, no solo al marcar \n",
    "    render=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de set de acciones\n",
    "\n",
    "Se importa el set de acciones de la misma biblioteca, se observa el espacio de estados y acciones posibles para el agente, los cuales tienen una dimensión de 115, y 21 correspondientemente. Se listan a continuación las acciones posibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gfootball.env import football_action_set\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "football_action_set.action_set_dict['default']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de bibliotecas restantes y definición del agente\n",
    "\n",
    "Se desactivan los notificaciones \"Warning\" del logging.\n",
    "Se importa el agente y se configura su espacio de estados y acciones para corresponder con el del entorno elegido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "from dqn_agent import Agent\n",
    "agent = Agent(state_size=115, action_size=21, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del agente\n",
    "\n",
    "Se especifican las condiciones de entrenamiento para el agente.\n",
    "El entrenamiento consistirá de 1000 episodios, en los cuales inicialmente el comportamiento del agente será 100% aleatorio, y disminuirá gradualemten hasta llegar a un 1% de comportamiento aleatorio. Luego el agente elegirá una acción, la implementará, almacenará el estado anterior, la acción tomada, la recompensa y el estado siguiente y se aprenderá. Este loop se repetirá hasta que el entorno devuelva un \"done\". \n",
    "\n",
    "Se guardan los puntajes obtenidos en cada paso, y además se priomedian los últimos 50 puntajes.\n",
    "Se imprime el progreso del entrenamiento, con los valores medios obtenidos en los últimos 50 episodios. \n",
    "Si el promedio obtenido es el mayor a todos los obtenidos hasta el momento, se guarda la red neuronal con el entrenamiento hasta el momento en un archivo .pth.\n",
    "(Es posible también determinar un valor promedio de puntuación esperado y guardar  la red entrenada cuando lo alcance y terminar el entrenamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episodes=5, batch_length=50, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): numero maximo de episodios de entrenamiento (n_episodios)\n",
    "        max_t (int): numero maximo de pasos por episodio (n_entrenamiento)\n",
    "        eps_start (float): valor inicial de epsilon\n",
    "        eps_end (float): valor final de epsilon\n",
    "        eps_decay (float): factor de multiplicacion (por episodio) de epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # puntuaciones de cada episodio\n",
    "    scores_window = deque(maxlen=batch_length)  # puntuaciones de los ultimos 100 episodios\n",
    "    mean_scores = []\n",
    "    eps = eps_start                    # inicializar epsilon\n",
    "    promedio = 0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        acc_reward = 0\n",
    "        while True:\n",
    "            \n",
    "            # elegir accion At con politica e-greedy\n",
    "            action = agent.act(state, eps)\n",
    "            \n",
    "            # aplicar At y obtener Rt+1, St+1\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            # almacenar <St, At, Rt+1, St+1>\n",
    "            agent.memory.add(state, action, reward, observation, done)\n",
    "            \n",
    "            # train & update\n",
    "            agent.step(state, action, reward, observation, done)\n",
    "            \n",
    "            acc_reward += reward \n",
    "            \n",
    "            if done:\n",
    "                break \n",
    "\n",
    "        scores_window.append(acc_reward)       # guardar ultima puntuacion\n",
    "        scores.append(acc_reward)              # guardar ultima puntuacion\n",
    "        eps = max(eps_end, eps_decay*eps) # reducir epsilon\n",
    "        \n",
    "        if len(scores_window)==batch_length:    # guardar historial de promedios\n",
    "            promedio = np.mean(scores_window) \n",
    "        mean_scores.append(promedio)\n",
    "        \n",
    "        print('\\rEpisodio {}\\tPuntuacion media (ultimos {:d}): {:.2f}'.format(i_episode, i_episode-int(i_episode/batch_length)*batch_length, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % batch_length == 0:\n",
    "            print('\\rEpisodio {}\\tPuntuacion media ({:d} anteriores): {:.2f}'.format(i_episode, batch_length, np.mean(scores_window)))\n",
    "#         if np.mean(scores_window)>=1.90:\n",
    "#             print('\\nProblema resuelto en {:d} episodios!\\tPuntuacion media (ultimos {:d}): {:.2f}'.format(i_episode-50, 50, np.mean(scores_window)))\n",
    "#             torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth') # guardar pesos de agente entrenado\n",
    "#             break\n",
    "        if len(scores_window)==batch_length and promedio >= max(mean_scores):\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "    return scores\n",
    "\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Puntuacion')\n",
    "plt.xlabel('Episodio #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente entrenado\n",
    "Implementación del agente entrenado durante 50 episodios.\n",
    "\n",
    "Se carga el archivo \"checkpoint.pth\" con la red entrenada previamente, y se pone al agente a actuar sin aleatoriedad durante 50 episodios. Se grafican luego los resultados obtenidos en cada episodio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[idle,\n",
       " left,\n",
       " top_left,\n",
       " top,\n",
       " top_right,\n",
       " right,\n",
       " bottom_right,\n",
       " bottom,\n",
       " bottom_left,\n",
       " long_pass,\n",
       " high_pass,\n",
       " short_pass,\n",
       " shot,\n",
       " sprint,\n",
       " release_direction,\n",
       " release_sprint,\n",
       " keeper_rush,\n",
       " release_keeper_rush,\n",
       " sliding,\n",
       " dribble,\n",
       " release_dribble]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gfootball.env as football_env\n",
    "env = football_env.create_environment(\n",
    "    env_name='academy_empty_goal_close', \n",
    "    stacked=False,                           # solo estado, no pixeles \n",
    "    representation='simple115',              # solo estado, no pixeles \n",
    "    with_checkpoints=True,                   # recompensas intermedias, no solo al marcar \n",
    "    render=True)   \n",
    "from gfootball.env import football_action_set\n",
    "football_action_set.action_set_dict['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "from dqn_agent import Agent\n",
    "agent = Agent(state_size=115, action_size=21, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 50\tPuntuacion media: 2.00\n",
      "puntuación media final: 2.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "scores = []                        # puntuaciones de cada episodio\n",
    "\n",
    "for i_episode in range(1, 51):\n",
    "    state = env.reset()\n",
    "    acc_reward = 0\n",
    "    while True:\n",
    "        # elegir accion At sin aleatoriedad\n",
    "        action = agent.act(state, 0.0)\n",
    "\n",
    "        # aplicar At y obtener Rt+1, St+1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        acc_reward += reward\n",
    "       \n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "    scores.append(acc_reward)              # guardar ultima puntuacion\n",
    "    print('\\rEpisodio {}\\tPuntuacion media: {:.2f}'.format(i_episode, np.mean(scores)), end=\"\")\n",
    "        \n",
    "env.close()\n",
    "\n",
    "print('\\npuntuación media final: %f' %np.mean(scores))\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores, '.')\n",
    "plt.ylabel('Puntuacion')\n",
    "plt.xlabel('Episodio #')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comentarios\n",
    "\n",
    "Se observa que eventualmente el agente aprende a patear en dirección al arco como mejor acción al alcanzar un valor medio de aproximadamente 1.80. Sin embargo, por el método aquí propuesto en el que se guarda la red neuronal entrenada siempre que mejore los valores obtenidos previamente, se observa que el agente mejoró su puntaje al correr en dirección al arco en vez de patear, ya que pateando había ocasiones en que no acertaba al arco. \n",
    "\n",
    "## Otros entornos\n",
    "\n",
    "Se intentó entrenar un agente en el entorno \"academy_3_vs_1_with_keeper\". Para esto se modificó la red neuronal utilizada, agregado 6 capas ocultas (las dos primeras de 256 nodos, las segundas de 128, y las 2 últimas de 64) sin éxito. Se intentó también aprovechar el aprendizaje del primer agente en el entorno \"empty_goal_close\" el cual ya sabía patear al arco, pero esto no resultó. Claramente los estados en estos dos entornos son muy distintos debido a la cantidad de jugadores con las que cuenta el agente, y la configuración inicial distinta que encuentra."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
